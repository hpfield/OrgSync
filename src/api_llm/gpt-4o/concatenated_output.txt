
# --- Start of main.py ---

import argparse
import sys
import os
import pickle
import json
import logging
from datetime import datetime
import asyncio

# Import stage functions
from stages.stage0 import stage0_check_new_data
from stages.stage1 import stage1_load_and_preprocess_data
from stages.stage2 import stage2_identify_identical_names
from stages.stage3 import stage3_vectorize_names
from stages.stage4 import stage4_group_similar_names
from stages.stage5 import stage5_perform_web_search
from stages.stage6 import stage6_process_groups_with_llm
from stages.stage7 import stage7_combine_overlapping_groups
from stages.stage8 import stage8_determine_organisation_type
from stages.stage9 import stage9_finalize_groups
from stages.stage10 import stage10_refine_groups_with_llm
from stages.stage11 import stage11_capitalize_group_names

def parse_arguments():
    parser = argparse.ArgumentParser(description='Process organization names in stages.')
    parser.add_argument('--stage', type=int, default=0, help='Stage to start from (0-11)')
    parser.add_argument('--threshold', type=float, default=0.5, help='Threshold for grouping similar names')
    parser.add_argument('--search-method', type=str, choices=['google', 'duckduckgo'], default='duckduckgo',
                        help='Method for web searching in Stage 5')
    parser.add_argument('--input', nargs='+', help='Input file(s) for the starting stage')
    parser.add_argument('--output-dir', type=str, default='outputs', help='Output directory to save results')
    parser.add_argument('--num-search-results', type=int, default=5,
                        help='Number of web search results to retrieve for each org name')
    parser.add_argument('--data-mode', type=str, choices=['all', 'new'], default='all',
                        help='Run pipeline over all data or only new data')
    return parser.parse_args()

def setup_logging(stage):
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_filename = os.path.join(log_dir, f"{timestamp}_stage{stage}.log")
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return timestamp

def main():
    args = parse_arguments()
    stage = args.stage
    input_files = args.input
    output_dir = args.output_dir
    timestamp = setup_logging(stage)

    os.makedirs(output_dir, exist_ok=True)

    # -----------
    # Stage 0: Load data
    # -----------
    if stage <= 0:
        repo_root = os.path.abspath(os.path.join(__file__, '../../../..'))
        logging.info(f'Repo root: {repo_root}')
        data_dir = os.path.join(repo_root, "data", "raw")
        new_data_path = os.path.join(repo_root, data_dir, "uk_data.json")
        old_data_path = os.path.join(repo_root, data_dir, "old_uk_data.json")
        merged_data_path = os.path.join(repo_root, data_dir, "merged_uk_data.json")
        new_entries_path = os.path.join(repo_root, data_dir, "new_entries.json")
        data_history_path = os.path.join(repo_root, data_dir, timestamp)
        merged_data = stage0_check_new_data(
            new_data_path,
            old_data_path,
            merged_data_path,
            new_entries_path
        )
        with open(os.path.join(output_dir, 'stage0_merged_data.pkl'), 'wb') as f:
            pickle.dump(merged_data, f)
        # Save data history
        os.makedirs(data_history_path, exist_ok=True)
        # Copy the old uk data to history log
        old_data_history_path = os.path.join(data_history_path, f"old_uk_data.json")
        os.system(f"cp {old_data_path} {old_data_history_path}")
        # Copy the new uk data to history log
        new_data_history_path = os.path.join(data_history_path, f"uk_data.json")
        os.system(f"cp {new_data_path} {new_data_history_path}")
        # Merged data becomes new old data
        os.system(f"cp {merged_data_path} {old_data_path}")
        logging.info("Stage 0 complete.")

    # ---------------------------
    # Stage 1: Load and preprocess data
    # ---------------------------
    if stage <= 1:
        if stage == 1 and input_files:
            with open(input_files[0], 'rb') as f:
                merged_data = pickle.load(f)
        else:
            with open(os.path.join(output_dir, 'stage0_merged_data.pkl'), 'rb') as f:
                merged_data = pickle.load(f)

        preprocessed_data = stage1_load_and_preprocess_data(merged_data)
        with open(os.path.join(output_dir, 'preprocessed_data.pkl'), 'wb') as f:
            pickle.dump(preprocessed_data, f)
        logging.info("Stage 1 complete.")

    # ---------------------------
    # Stage 2: Identify identical names
    # ---------------------------
    if stage <= 2:
        if stage == 2 and input_files:
            with open(input_files[0], 'rb') as f:
                preprocessed_data = pickle.load(f)
        else:
            with open(os.path.join(output_dir, 'preprocessed_data.pkl'), 'rb') as f:
                preprocessed_data = pickle.load(f)

        identical_name_groups = stage2_identify_identical_names(preprocessed_data)
        with open(os.path.join(output_dir, 'identical_name_groups_stage2.json'), 'w') as f:
            json.dump(identical_name_groups, f, indent=2)
        logging.info("Stage 2 complete.")

    # ---------------------------
    # Stage 3: Vectorize names
    # ---------------------------
    if stage <= 3:
        if stage == 3 and input_files:
            with open(input_files[0], 'rb') as f:
                preprocessed_data = pickle.load(f)
        else:
            with open(os.path.join(output_dir, 'preprocessed_data.pkl'), 'rb') as f:
                preprocessed_data = pickle.load(f)

        vectorizer, name_vectors, unique_entries = stage3_vectorize_names(preprocessed_data)
        with open(os.path.join(output_dir, 'vectorizer_stage3.pkl'), 'wb') as f:
            pickle.dump(vectorizer, f)
        with open(os.path.join(output_dir, 'name_vectors_stage3.pkl'), 'wb') as f:
            pickle.dump(name_vectors, f)
        with open(os.path.join(output_dir, 'unique_entries_stage3.pkl'), 'wb') as f:
            pickle.dump(unique_entries, f)
        with open(os.path.join(output_dir, 'unique_entries_stage3.json'), 'w') as f:
            json.dump(unique_entries, f, indent=2)
        logging.info("Stage 3 complete.")

    # ---------------------------
    # Stage 4: Group similar names
    # ---------------------------
    if stage <= 4:
        if stage == 4 and input_files:
            with open(input_files[0], 'rb') as f:
                vectorizer = pickle.load(f)
            with open(input_files[1], 'rb') as f:
                name_vectors = pickle.load(f)
            with open(input_files[2], 'rb') as f:
                unique_entries = pickle.load(f)
        else:
            with open(os.path.join(output_dir, 'vectorizer_stage3.pkl'), 'rb') as f:
                vectorizer = pickle.load(f)
            with open(os.path.join(output_dir, 'name_vectors_stage3.pkl'), 'rb') as f:
                name_vectors = pickle.load(f)
            with open(os.path.join(output_dir, 'unique_entries_stage3.pkl'), 'rb') as f:
                unique_entries = pickle.load(f)

        grouped_names = stage4_group_similar_names(
            vectorizer, name_vectors, unique_entries, threshold=args.threshold
        )

        # For new data, filter groups to only include those with new entries;
        # for "all", pass along all groups.
        if args.data_mode == "new":
            new_data_groups = {}
            for rep_name, group_info in grouped_names.items():
                items = group_info["items"]
                if any(item.get("is_new", False) for item in items):
                    new_data_groups[rep_name] = group_info
            logging.info(f"Created {len(new_data_groups)} groups with new data.")
            if len(new_data_groups) == 0:
                logging.info("No groups contain new data. Exiting pipeline.")
                sys.exit(0)
            grouped_names_stage4_path = os.path.join(output_dir, 'grouped_names_stage4_new_data_only.json')
            with open(grouped_names_stage4_path, 'w') as f:
                json.dump(new_data_groups, f, indent=2)
        else:
            grouped_names_stage4_path = os.path.join(output_dir, 'grouped_names_stage4_all_data.json')
            with open(grouped_names_stage4_path, 'w') as f:
                json.dump(grouped_names, f, indent=2)
        logging.info("Stage 4 complete.")

    # ---------------------------
    # Stage 5: Perform web search
    # ---------------------------
    if stage <= 5:
        if stage == 5 and input_files:
            with open(input_files[0], 'r') as f:
                grouped_names = json.load(f)
        else:
            with open(grouped_names_stage4_path, 'r') as f:
                grouped_names = json.load(f)
            with open(os.path.join(output_dir, 'unique_entries_stage3.json'), 'r') as f:
                unique_entries = json.load(f)

        all_web_results = stage5_perform_web_search(
            grouped_names,
            unique_entries,
            search_method=args.search_method,
            num_results=args.num_search_results,
            output_dir=output_dir
        )

        sub_db = all_web_results.get(args.search_method, {})
        with open(os.path.join(output_dir, 'web_search_results_stage5.json'), 'w') as f:
            json.dump(sub_db, f, indent=2)
        logging.info("Stage 5 complete.")

    # ---------------------------
    # Stage 6: Process groups with LLM
    # ---------------------------
    if stage <= 6:
        refined_groups_out = os.path.join(output_dir, 'refined_groups_stage6.json')
        if stage == 6 and input_files:
            with open(input_files[0], 'r') as f:
                grouped_names = json.load(f)
            with open(input_files[1], 'r') as f:
                method_sub_db = json.load(f)
        else:
            if args.data_mode == "new":
                with open(os.path.join(output_dir, 'grouped_names_stage4_new_data_only.json'), 'r') as f:
                    grouped_names = json.load(f)
            else:
                with open(os.path.join(output_dir, 'grouped_names_stage4_all_data.json'), 'r') as f:
                    grouped_names = json.load(f)
            with open(os.path.join(output_dir, 'web_search_results_stage5.json'), 'r') as f:
                method_sub_db = json.load(f)

        # refined_groups = asyncio.run(stage6_process_groups_with_llm(grouped_names, method_sub_db))
        refined_groups = stage6_process_groups_with_llm(grouped_names, method_sub_db)
        with open(refined_groups_out, 'w') as f:
            json.dump(refined_groups, f, indent=2)
        logging.info("Stage 6 complete.")

    # ---------------------------
    # Stage 7: Combine overlapping groups
    # ---------------------------
    if stage <= 7:
        refined_groups_path = os.path.join(output_dir, 'refined_groups_stage6.json')
        with open(refined_groups_path, 'r') as f:
            refined_groups = json.load(f)

        merged_groups = stage7_combine_overlapping_groups(refined_groups)
        with open(os.path.join(output_dir, 'merged_groups_stage7.json'), 'w') as f:
            json.dump(merged_groups, f, indent=2)
        logging.info("Stage 7 complete.")

    # ---------------------------
    # Stage 8: Determine organisation type
    # ---------------------------
    if stage <= 8:
        merged_groups_path = os.path.join(output_dir, 'merged_groups_stage7.json')
        with open(merged_groups_path, 'r') as f:
            merged_groups = json.load(f)
        with open(os.path.join(output_dir, 'web_search_results_stage5.json'), 'r') as f:
            method_sub_db = json.load(f)

        groups_with_types = stage8_determine_organisation_type(merged_groups, method_sub_db)
        with open(os.path.join(output_dir, 'groups_with_types_stage8.json'), 'w') as f:
            json.dump(groups_with_types, f, indent=2)
        logging.info("Stage 8 complete.")

    # ---------------------------
    # Stage 9: Finalize groups (produce formatted groups)
    # ---------------------------
    if stage <= 9:
        groups_with_types_path = os.path.join(output_dir, 'groups_with_types_stage8.json')
        with open(groups_with_types_path, 'r') as f:
            groups_with_types = json.load(f)
        with open(os.path.join(output_dir, 'web_search_results_stage5.json'), 'r') as f:
            method_sub_db = json.load(f)
        with open(os.path.join(output_dir, 'identical_name_groups_stage2.json'), 'r') as f:
            identical_name_groups = json.load(f)
        with open(os.path.join(output_dir, 'unique_entries_stage3.json'), 'r') as f:
            unique_entries = json.load(f)

        formatted_groups = stage9_finalize_groups(
            groups_with_types, method_sub_db, unique_entries
        )

        # Save as formatted groups (do not merge to rolling output yet)
        formatted_groups_path = os.path.join(output_dir, 'formatted_groups_stage9.json')
        with open(formatted_groups_path, 'w') as f:
            json.dump(formatted_groups, f, indent=2)
        logging.info("Stage 9 complete (formatted groups saved).")

    # ---------------------------
    # Stage 10: Refine groups with LLM (post-stage 9)
    # ---------------------------
    if stage <= 10:
        formatted_groups_path = os.path.join(output_dir, 'formatted_groups_stage9.json')
        with open(formatted_groups_path, 'r') as f:
            formatted_groups = json.load(f)
        with open(os.path.join(output_dir, 'web_search_results_stage5.json'), 'r') as f:
            method_sub_db = json.load(f)
        with open(os.path.join(output_dir, 'unique_entries_stage3.json'), 'r') as f:
            unique_entries = json.load(f)

        refined_groups = stage10_refine_groups_with_llm(formatted_groups, method_sub_db, unique_entries)
        refined_groups_path = os.path.join(output_dir, 'refined_groups_stage10.json')
        with open(refined_groups_path, 'w') as f:
            json.dump(refined_groups, f, indent=2)
        logging.info("Stage 10 complete.")

    # ---------------------------
    # Stage 11: Apply capitalisation to group names and merge to rolling output
    # ---------------------------
    if stage <= 11:
        refined_groups_path = os.path.join(output_dir, 'refined_groups_stage10.json')
        with open(refined_groups_path, 'r') as f:
            groups_to_capitalise = json.load(f)
        with open(os.path.join(output_dir, 'web_search_results_stage5.json'), 'r') as f:
            method_sub_db = json.load(f)
        final_groups = stage11_capitalize_group_names(groups_to_capitalise, method_sub_db)
        final_groups_path = os.path.join(output_dir, 'final_groups_stage11.json')
        with open(final_groups_path, 'w') as f:
            json.dump(final_groups, f, indent=2)
        logging.info("Stage 11 complete.")

        # Merge with rolling output (output_groups.json)
        rolling_path = os.path.join(output_dir, 'output_groups.json')
        if os.path.exists(rolling_path):
            with open(rolling_path, 'r') as f:
                old_final = json.load(f)
        else:
            old_final = {}
        old_final.update(final_groups)
        with open(rolling_path, 'w') as f:
            json.dump(old_final, f, indent=2)
        logging.info(f"Updated rolling final output at {rolling_path}.")

if __name__ == '__main__':
    main()

# --- End of main.py ---


# --- Start of stages/__init__.py ---


# --- End of stages/__init__.py ---


# --- Start of stages/stage0.py ---

import json
import os
import logging

logger = logging.getLogger(__name__)

def stage0_check_new_data(new_data_path, old_data_path, merged_data_path, new_entries_path):
    """
    - Loads old_data if it exists,
    - Loads new_data,
    - Finds the 'new entries' that exist in new_data but not in old_data,
    - Writes out:
        1) A merged dataset (old + new) to `merged_data_path`
        2) The list of truly new entries to `new_entries_path`.

    Returns the merged_data (list of dicts) so it can be passed to subsequent stages.
    """

    # Load new data
    with open(new_data_path, 'r', encoding='utf-8') as f:
        new_data = json.load(f)

    # Attempt to load old data
    if os.path.exists(old_data_path):
        with open(old_data_path, 'r', encoding='utf-8') as f:
            old_data = json.load(f)
        logger.info(f"Loaded old database with {len(old_data)} entries from {old_data_path}")
    else:
        old_data = []
        logger.info(f"No old database found at {old_data_path}; starting fresh.")

    # Convert each record to a frozenset of items so we can deduplicate
    old_data_set = {frozenset(d.items()) for d in old_data}
    new_data_set = {frozenset(d.items()) for d in new_data}

    # Identify truly new entries
    truly_new_entries_set = new_data_set - old_data_set
    truly_new_entries = [dict(fs) for fs in truly_new_entries_set]

    # Merge old + new
    merged_data_set = old_data_set | new_data_set
    merged_data = [dict(fs) for fs in merged_data_set]

    logger.info(f"Merged dataset has {len(merged_data)} total entries.")
    logger.info(f"Found {len(truly_new_entries)} new entries not in the old database.")

    # Mark is_new=True on truly new, else False
    for item in merged_data:
        if frozenset(item.items()) in truly_new_entries_set:
            item["is_new"] = True
        else:
            item["is_new"] = False

    # Save the merged dataset
    with open(merged_data_path, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, indent=4)
    logger.info(f"Merged data written to {merged_data_path}")

    # Save new entries (if you want to track them separately)
    with open(new_entries_path, 'w', encoding='utf-8') as f:
        json.dump(truly_new_entries, f, indent=4)
    logger.info(f"New entries written to {new_entries_path}")

    return merged_data

# --- End of stages/stage0.py ---


# --- Start of stages/stage1.py ---

import json
import re
import logging

logger = logging.getLogger(__name__)

def stage1_load_and_preprocess_data(data=None):
    """
    Load and preprocess the merged data (or fallback to reading from disk).
    Convert 'name' + 'short_name' into a single 'combined_name' field.
    Preserve 'is_new' so that we can identify new entries later.
    """

    if data is None:
        # fallback to reading from disk -- if needed
        with open('/home/ubuntu/OrgSync/data/raw/uk_data.json', 'r') as file:
            data = json.load(file)

    def preprocess_name(name):
        name = name.lower()
        name = re.sub(r'\s+', ' ', name)
        name = re.sub(r'[^\w\s]', '', name)
        return name.strip()

    def combine_entry(entry):
        combined_name = preprocess_name(
            ' '.join(filter(None, [entry.get('name', ''), entry.get('short_name', '')]))
        )
        return {
            "combined_name": combined_name,
            "dataset": entry.get("dataset", ""),
            "unique_id": entry.get("unique_id", ""),
            "postcode": entry.get("postcode", ""),
            # Carry over the is_new flag if present, else False
            "is_new": entry.get("is_new", False),
        }

    preprocessed_data = [combine_entry(entry) for entry in data]
    logger.info(f"Loaded and preprocessed {len(preprocessed_data)} entries.")
    return preprocessed_data

# --- End of stages/stage1.py ---


# --- Start of stages/stage10.py ---

import json
import logging
from tqdm import tqdm
from pydantic import BaseModel
from stages.utils import UserMessage, SystemMessage, get_client

logger = logging.getLogger(__name__)

class RefineGroupResponse(BaseModel):
    selected_names: list[str]

def stage10_refine_groups_with_llm(formatted_groups, web_search_results, unique_entries):
    client = get_client()
    refined_results = {}
    
    # Build lookup for unique entries by lower-case combined_name
    names_lookup = {}
    for entry in unique_entries:
        key = entry["combined_name"].lower()
        item = {
            "org_name": entry["combined_name"],
            "unique_id": entry["unique_id"],
            "dataset": entry["dataset"],
            "postcode": entry["postcode"]
        }
        names_lookup.setdefault(key, []).append(item)
    
    logger.info("Refining formatted groups with LLM...")#
    logging.getLogger("openai").setLevel(logging.ERROR)
    logging.getLogger("httpx").setLevel(logging.ERROR)
    for group_id, group_info in tqdm(formatted_groups.items(), desc="Refining groups"):
        rep_name = group_info.get("name", "").lower()
        organisation_type = group_info.get("organisation_type", "")
        
        # Derive candidate names from the group's items using the canonical 'org_name' field.
        candidate_names_set = set()
        for item in group_info.get("items", []):
            if "org_name" in item:
                candidate_names_set.add(item["org_name"].lower())
        candidate_names = list(candidate_names_set)
        
        if not candidate_names:
            logger.info(f"Skipping group {rep_name} due to no candidate names found.")
            continue
        
        # Build web search results context for each candidate name.
        web_results_str = ""
        for name in candidate_names:
            results = web_search_results.get(name, [])
            if results:
                result_strs = []
                for result in results:
                    url = result.get('url', '')
                    title = result.get('title', '')
                    description = result.get('description', '')
                    result_strs.append(f"Title: {title}\nURL: {url}\nDescription: {description}")
                combined = "\n\n".join(result_strs)
                web_results_str += f"Search results for '{name}':\n{combined}\n\n"
            else:
                web_results_str += f"Search results for '{name}':\nNo results available.\n\n"
        
        prompt = f"""Given the following candidate organization names:
{', '.join(candidate_names)}

The chosen representative name is "{rep_name}".
All are determined to be a/an {organisation_type}.
Based on the web search results below, please select the names that truly belong to the same organization as "{rep_name}", 
and output them as a JSON object with a key "selected_names" that is an array of names in lowercase.
Exclude any ambiguous names. No extra text, just JSON.
Web search results:
{web_results_str}
"""
        system_message = "You are an AI assistant that refines organization groups based on provided candidate names, organisation type, and web search results. Output must be a JSON object with a key 'selected_names' containing a list of valid names in lowercase, with no additional text."
        chat_history = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": prompt}
        ]
        try:
            result = client.beta.chat.completions.parse(
                model="gpt-4o",
                messages=chat_history,
                response_format=RefineGroupResponse,
            )
            refined_names = result.choices[0].message.parsed.selected_names
            if not isinstance(refined_names, list):
                logger.warning(f"LLM response is not a list for group {rep_name}.")
                refined_names = []
        except Exception as e:
            logger.error(f"Error processing group {rep_name} with LLM: {e}")
            refined_names = []
        
        # Ensure the representative name is included.
        if rep_name not in refined_names:
            refined_names.append(rep_name)
        
        # Build refined items for the group from the preprocessed lookup.
        items_for_group = []
        for name in refined_names:
            normalized_name = name.lower()
            if normalized_name in names_lookup:
                items_for_group.extend(names_lookup[normalized_name])
        
        if len(items_for_group) > 1:
            refined_results[group_id] = {
                "name": group_info["name"],
                "items": items_for_group
            }
        else:
            logger.info(f"Skipping group {rep_name} due to insufficient items after LLM refinement.")
    logger.info(f"Refined groups count: {len(refined_results)}")
    return refined_results

# --- End of stages/stage10.py ---


# --- Start of stages/stage11.py ---

import json
import logging
from tqdm import tqdm
from pydantic import BaseModel
from stages.utils import UserMessage, SystemMessage, get_client

logger = logging.getLogger(__name__)

class CapitalisedNameResponse(BaseModel):
    capitalised_name: str

def stage11_capitalize_group_names(groups, web_search_results):
    client = get_client()
    capitalised_groups = {}
    logger.info("Applying capitalisation to group names via LLM...")
    logging.getLogger("openai").setLevel(logging.ERROR)
    logging.getLogger("httpx").setLevel(logging.ERROR)

    for group_id, group_info in tqdm(groups.items(), desc="Capitalising group names"):
        current_name = group_info["name"]

        # Gather web search results context for the current representative name.
        web_results = web_search_results.get(current_name.lower(), [])
        web_context = ""
        if web_results:
            result_strs = []
            for result in web_results:
                url = result.get('url', '')
                title = result.get('title', '')
                description = result.get('description', '')
                result_strs.append(f"Title: {title}\nURL: {url}\nDescription: {description}")
            web_context = "\n\n".join(result_strs)
        
        prompt = f"""Please provide the correct capitalisation for the organization name below, without altering its spelling:
"{current_name}"

Here is some context from web search results:
{web_context}

Note:
- It is highly unlikely that the company name should be in all caps.
- Ignore any instances of full caps as provided by Companies House.
- Do not change the spelling of the representative name; only adjust the capitalisation.

Output the result as JSON in the format: {{"capitalised_name": "Proper Capitalisation"}} with no additional text.
"""
        system_message = "You are an AI assistant that only corrects the capitalisation of organization names based on provided context, without changing the spelling."
        chat_history = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": prompt}
        ]
        
        try:
            result = client.beta.chat.completions.parse(
                model="gpt-4o",
                messages=chat_history,
                response_format=CapitalisedNameResponse,
            )
            capitalised_name = result.choices[0].message.parsed.capitalised_name
            if not capitalised_name:
                capitalised_name = current_name
        except Exception as e:
            logger.error(f"Error in capitalisation for group {group_id}: {e}")
            capitalised_name = current_name
        
        capitalised_groups[group_id] = {
            "name": capitalised_name,
            "items": group_info["items"]
        }
    logger.info("Capitalisation stage complete.")
    return capitalised_groups

# --- End of stages/stage11.py ---


# --- Start of stages/stage2.py ---

import logging
from collections import defaultdict

logger = logging.getLogger(__name__)

def stage2_identify_identical_names(preprocessed_data):
    """
    Identify groups of entries that share the exact same 'combined_name'.
    Return a dict: { <combined_name>: [list_of_entries_with_that_name], ... }
    excluding any groups of size 1.
    """
    name_groups = defaultdict(list)
    for entry in preprocessed_data:
        name_groups[entry["combined_name"]].append(entry)

    # Filter out single-entry groups
    multi_name_groups = {name: entries for name, entries in name_groups.items() if len(entries) > 1}

    logger.info(f"Found {len(multi_name_groups)} identical-name groups with more than 1 entry.")
    return multi_name_groups

# --- End of stages/stage2.py ---


# --- Start of stages/stage3.py ---

import logging
from sklearn.feature_extraction.text import TfidfVectorizer

logger = logging.getLogger(__name__)

def stage3_vectorize_names(preprocessed_data):
    """
    Converts each dictionary entry to a frozenset so we only remove truly duplicate
    dictionaries. Then vectorizes the 'combined_name' field for similarity checks later.
    """
    # Make entire entry hashable -> remove exact duplicates
    unique_entries = list({frozenset(entry.items()): entry for entry in preprocessed_data}.values())

    # Extract the combined names for vectorization
    unique_combined_names = [entry['combined_name'] for entry in unique_entries]
    total_unique_entries = len(unique_entries)

    # Vectorize those combined names
    vectorizer = TfidfVectorizer().fit(unique_combined_names)
    name_vectors = vectorizer.transform(unique_combined_names)

    logger.info(f"Vectorized {total_unique_entries} unique entries based on full uniqueness.")
    return vectorizer, name_vectors, unique_entries

# --- End of stages/stage3.py ---


# --- Start of stages/stage4.py ---

import logging
from collections import defaultdict
from sklearn.neighbors import NearestNeighbors

logger = logging.getLogger(__name__)

def stage4_group_similar_names(vectorizer, name_vectors, unique_entries, threshold=0.5):
    """
    Groups 'unique_entries' whose 'combined_name' fields are similar
    based on the TF-IDF vectors and a cosine distance threshold.
    
    Returns a dict of the form:
    {
       <representative_name>: {
          "matched_names": [other similar names],
          "items": [the full item dicts (representative + matched)]
       },
       ...
    }
    """

    # We'll need quick access from combined_name -> item dicts
    name_to_itemlist = defaultdict(list)
    for entry in unique_entries:
        nm = entry["combined_name"]
        name_to_itemlist[nm].append(entry)

    all_names = [entry["combined_name"] for entry in unique_entries]
    nbrs = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute').fit(name_vectors)
    distances, indices = nbrs.kneighbors(name_vectors)

    grouped_names = {}
    used_names = set()

    for i, name in enumerate(all_names):
        if name in used_names:
            continue

        similar_names = []
        for j, idx in enumerate(indices[i]):
            if distances[i][j] <= threshold and idx != i:
                neighbor_name = all_names[idx]
                if neighbor_name not in used_names:
                    similar_names.append(neighbor_name)

        all_group_names = [name] + similar_names
        if len(all_group_names) > 1:
            # Mark them as 'used'
            for gnm in all_group_names:
                used_names.add(gnm)

            grouped_names[name] = {
                "matched_names": similar_names,
                # Gather all the item dicts from each group name
                "items": [
                    item
                    for gnm in all_group_names
                    for item in name_to_itemlist[gnm]
                ],
            }
        else:
            # No match found under threshold, skip adding a group for size=1
            # (or handle singletons if you want them)
            continue

    logger.info(f"Grouped names into {len(grouped_names)} groups (size >= 2).")
    return grouped_names

# --- End of stages/stage4.py ---


# --- Start of stages/stage5.py ---

import logging
import json
import os
from tqdm import tqdm
from stages.utils import perform_web_search

logger = logging.getLogger(__name__)

def stage5_perform_web_search(grouped_names, all_names_and_items, search_method='duckduckgo', num_results=3, output_dir='outputs'):
    """
    - Maintains a rolling database of all web searches in 'all_web_search_results.json'.
    - For the given search_method, only fetch new results if we have < num_results for that name (or it's absent).
    - Returns the entire dictionary, but the pipeline typically uses only the sub-dict for the active method.
    
    Modification:
      - Use the postcode from all_names_and_items to form the search query as '{name} {postcode}'
    """

    if search_method == 'duckduckgo':
        logger.info(f"Due to rate limits, the program will wait for 60 seconds every 20 searches.")

    # Path to the rolling DB
    rolling_db_path = os.path.join(output_dir, 'all_web_search_results.json')

    # Load existing DB if it exists; else initialize
    if os.path.exists(rolling_db_path):
        with open(rolling_db_path, 'r') as f:
            all_web_search_results = json.load(f)
        logger.info(f"Loaded existing web search DB from {rolling_db_path}")
    else:
        all_web_search_results = {}
        logger.info("Initializing new web search DB.")

    # Ensure we have a sub-dict for this search method
    if search_method not in all_web_search_results:
        logger.info(f"Couldn't find any previous search results using {search_method}")
        all_web_search_results[search_method] = {}

    # Pre-process all_names_and_items to build a lookup of normalized name -> postcode
    postcode_lookup = {}
    for entry in all_names_and_items:
        key = entry["combined_name"].lower()
        # Use the first postcode encountered for this name
        if key not in postcode_lookup:
            postcode_lookup[key] = entry["postcode"]

    # Extract all names from grouped_names
    unique_names = set()
    for rep_name, info in grouped_names.items():
        unique_names.add(rep_name)
        for nm in info["matched_names"]:
            unique_names.add(nm)

    # We'll gather a list of names that need searching
    names_to_search = []
    for name in unique_names:
        existing_results = all_web_search_results[search_method].get(name, [])
        # If we have fewer than num_results, we do a fresh search
        if len(existing_results) < num_results:
            names_to_search.append(name)

    logger.info(f"{len(names_to_search)} out of {len(unique_names)} names need new web searches.")

    # Perform the new/updated searches
    for name in tqdm(names_to_search, desc='Performing new web searches'):
        # Look up the postcode for the name (using a case-insensitive key)
        normalized_name = name.lower()
        postcode = postcode_lookup.get(normalized_name, "").strip()
        # Build the query with the postcode if available
        if postcode:
            query = f"{name} {postcode}"
        else:
            query = name

        try:
            # Perform search with the constructed query
            new_results_dict = perform_web_search([query], num_results=num_results, search_method=search_method)
            # The results are expected to be in the dict with the key as our query
            found = new_results_dict.get(query, [])
            # Update the DB entry using the original name as the key
            all_web_search_results[search_method][name] = found
        except Exception as e:
            logger.error(f"Error searching for '{name}': {e}")
            # Keep old results if something fails

    # Save the updated rolling DB
    with open(rolling_db_path, 'w') as f:
        json.dump(all_web_search_results, f, indent=2)
    logger.info(f"Updated rolling DB saved to {rolling_db_path}")

    return all_web_search_results

# --- End of stages/stage5.py ---


# --- Start of stages/stage6.py ---

import json
import logging
from tqdm import tqdm  # synchronous progress bar
from stages.utils import get_client
from stages.utils import GroupResponse

logger = logging.getLogger(__name__)

def stage6_process_groups_with_llm(grouped_names, web_search_results):
    """
    Receives `grouped_names` in the format:
      {
        "<representative_name>": {
          "matched_names": [...],
          "items": [...]
        },
        ...
      }
    along with a web_search_results dict mapping each name -> list of search hits.

    Calls the LLM synchronously to decide which of the 'matched_names' belong to the same
    organization as the unique name. The result is stored in `refined_groups` as:
      refined_groups[unique_name] = [list_of_selected_names]
    """
    logging.getLogger("openai").setLevel(logging.ERROR)
    logging.getLogger("httpx").setLevel(logging.ERROR)
    original_level = logger.getEffectiveLevel()
    logger.setLevel(logging.ERROR)
    client = get_client()  # Assumed now to support synchronous calls

    refined_groups = {}
    num_groups = len(grouped_names)

    logger.info("Processing groups with LLM synchronously...")

    pbar = tqdm(total=num_groups, desc='Processing groups with LLM')
    for unique_name, info in grouped_names.items():
        matched_names_list = info.get("matched_names", [])
        # Build a dict mapping each name in the group to its search results.
        group_names = [unique_name] + matched_names_list
        group_search_results = {
            name: web_search_results.get(name, [])
            for name in group_names
        }
        try:
            unique_name, response = process_group_with_llm(unique_name, matched_names_list, group_search_results, client)
        except Exception as e:
            logger.exception(f"Processing group '{unique_name}' raised an exception: {e}")
            continue

        pbar.update(1)

        # Parse the response (expected to be a JSON array string)
        try:
            if isinstance(response, list):
                selected_names = response
            else:
                logger.warning(f"LLM response for group '{unique_name}' is not a list. Response was: {response}")
                selected_names = []
        except json.JSONDecodeError:
            logger.error(f"Error parsing LLM response for group '{unique_name}': {response}")
            selected_names = []
        except Exception as e:
            logger.exception(f"Unexpected error processing group '{unique_name}': {e}")
            selected_names = []

        # Ensure the original unique_name is included
        if unique_name not in selected_names:
            selected_names.append(unique_name)

        refined_groups[unique_name] = selected_names

    pbar.close()

    logger.setLevel(original_level)
    logger.info(f"Refined groups to {len(refined_groups)} groups after LLM processing.")
    return refined_groups

def process_group_with_llm(unique_name, matched_names_list, group_search_results, client):
    """
    Synchronously sends a prompt to the LLM:
      - unique_name is the 'primary' name.
      - matched_names_list are possible duplicates.
      - group_search_results is a dict {org_name -> list_of_search_hits}.
      - client is the local LLM client object.

    Expects the LLM to return a JSON array (as a string) of selected names in lowercase.
    Returns a tuple (unique_name, response).
    """
    system_message = (
        "You are an AI assistant that helps identify whether organization names refer "
        "to the same UK research organization. You have access to web search results to assist. "
        "Output must be a JSON array of selected names in lowercase, no extra text."
    )
        
    # Build a text block summarizing all search results
    web_results_str = ""
    all_names = [unique_name] + matched_names_list
    for name in all_names:
        results = group_search_results.get(name, [])
        if results:
            result_strs = []
            for result in results:
                url = result.get('url', '')
                title = result.get('title', '')
                description = result.get('description', '')
                result_strs.append(f"Title: {title}\nURL: {url}\nDescription: {description}")
            combined = '\n\n'.join(result_strs)
            web_results_str += f"Search results for '{name}':\n{combined}\n\n"
        else:
            web_results_str += f"Search results for '{name}':\nNo results currently available.\n\n"

    matched_names_str = '\n'.join(matched_names_list)
    prompt = f"""Given the organization name: "{unique_name}"
and the following list of similar names:
{matched_names_str}

Here are web search results for each name:
{web_results_str}

Please select the names that belong to the same organization as "{unique_name}", 
and output them as a JSON array in lowercase. If ambiguous, exclude the name.
No extra text, just JSON array (e.g. ["acme corp", "acme inc"])."""

    chat_history = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
    ]

    # Synchronously call the API using the blocking method.
    completion = client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=chat_history,
        response_format=GroupResponse, 
    )

    # Extract and return the response from the first choice.
    response = completion.choices[0].message.parsed
    return unique_name, response.selected_names

# --- End of stages/stage6.py ---


# --- Start of stages/stage7.py ---

import logging

logger = logging.getLogger(__name__)

def stage7_combine_overlapping_groups(refined_groups):
    logger.info("Combining overlapping groups...")
    group_sets = []
    for names in refined_groups.values():
        # Ensure each group is a set of strings
        if not isinstance(names, list):
            logger.warning(f"Expected a list, got {type(names)}. Converting to empty list.")
            names = []
        else:
            names = [str(name).strip() for name in names]
        group_sets.append(set(names))

    merged_groups = merge_overlapping_groups(group_sets)
    # Convert each set to a sorted list
    merged_groups = [sorted(list(g)) for g in merged_groups]
    logger.info(f"Number of combined groups: {len(merged_groups)}")
    return merged_groups

def merge_overlapping_groups(group_sets):
    merged = []
    for group in group_sets:
        found = False
        for mgroup in merged:
            if group & mgroup:
                mgroup |= group
                found = True
                break
        if not found:
            merged.append(set(group))

    # Repeatedly merge if new overlaps are introduced
    merging = True
    while merging:
        merging = False
        for i in range(len(merged)):
            for j in range(i + 1, len(merged)):
                if merged[i] & merged[j]:
                    merged[i] |= merged[j]
                    del merged[j]
                    merging = True
                    break
            if merging:
                break
    return merged

# --- End of stages/stage7.py ---


# --- Start of stages/stage8.py ---

import json
import logging
from tqdm import tqdm
from pydantic import BaseModel
from stages.utils import UserMessage, SystemMessage, get_client

logger = logging.getLogger(__name__)

class OrgTypeResponse(BaseModel):
    organisation_type: str

def stage8_determine_organisation_type(merged_groups, web_search_results):
    client = get_client()
    groups_with_types = []
    logger.info("Determining organisation types for groups...")
    logger.info(f'lenght of merged_groups: {len(merged_groups)}')
    logging.getLogger("openai").setLevel(logging.ERROR)
    logging.getLogger("httpx").setLevel(logging.ERROR)

    with tqdm(total=len(merged_groups), desc='Determining org types') as pbar:
        for group_names in merged_groups:
            pbar.update(1)
            group_search_results = {name: web_search_results.get(name, []) for name in group_names}
            response = determine_organisation_type_with_llm(group_names, group_search_results, client)
            try:
                # response is already parsed into our OrgTypeResponse model.
                organisation_type = response.organisation_type
            except Exception as e:
                logger.exception(f"Error processing LLM response. Response was: {response}, error: {e}")
                organisation_type = ''
            groups_with_types.append({
                'group_names': group_names,
                'organisation_type': organisation_type
            })

    logger.info(f"Completed organisation type detection for {len(groups_with_types)} groups.")
    return groups_with_types

def determine_organisation_type_with_llm(group_names, group_search_results, client):
    system_message = "You are an AI assistant that helps identify the type of organisation " \
            "these group names refer to. Output must be JSON with a single key 'organisation_type'."

    web_results_str = ""
    for name in group_names:
        results = group_search_results.get(name, [])
        if results:
            entries = []
            for res in results:
                url = res.get('url', '')
                title = res.get('title', '')
                desc = res.get('description', '')
                entries.append(f"Title: {title}\nURL: {url}\nDescription: {desc}")
            combined = "\n\n".join(entries)
            web_results_str += f"Search results for '{name}':\n{combined}\n\n"
        else:
            web_results_str += f"Search results for '{name}':\nNo results found.\n\n"

    prompt = f"""
Given this group of organisation names:
{', '.join(group_names)}

And the following web search results:
{web_results_str}

Classify the type of organisation (e.g. 'company', 'university', 'government', 'non-profit', etc.).
Output JSON of the form:

{{"organisation_type": "some type"}}
"""
    # user_message = UserMessage(content=prompt)
    chat_history = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
    ]

    completion = client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=chat_history,
        response_format=OrgTypeResponse,
    )

    return completion.choices[0].message.parsed

# --- End of stages/stage8.py ---


# --- Start of stages/stage9.py ---

import json
import logging
import uuid
from tqdm import tqdm
from pydantic import BaseModel
from stages.utils import UserMessage, SystemMessage, get_client

logger = logging.getLogger(__name__)

class RepresentativeNameResponse(BaseModel):
    name: str

def stage9_finalize_groups(groups_with_types, web_search_results, all_names_and_items):
    client = get_client()
    final_results = {}
    logger.info("Finalizing groups to produce the formatted groups with group UUIDs...")
    logging.getLogger("openai").setLevel(logging.ERROR)
    logging.getLogger("httpx").setLevel(logging.ERROR)

    # Build lookup for unique entries by lower-case combined_name
    names_lookup = {}
    for entry in all_names_and_items:
        key = entry["combined_name"].lower()
        item = {
            "org_name": entry["combined_name"],
            "unique_id": entry["unique_id"],
            "dataset": entry["dataset"],
            "postcode": entry["postcode"]
        }
        names_lookup.setdefault(key, []).append(item)

    with tqdm(total=len(groups_with_types), desc='Finalizing groups') as pbar:
        for group_info in groups_with_types:
            pbar.update(1)
            organisation_type = group_info.get('organisation_type', '')
            # Derive candidate names either from provided "group_names" or from items.
            candidate_names_set = set()
            if "group_names" in group_info:
                candidate_names_set.update(name.lower() for name in group_info["group_names"])
            else:
                for item in group_info.get("items", []):
                    candidate_names_set.add(item.get("org_name", "").lower())
            candidate_names = list(candidate_names_set)
            if not candidate_names:
                continue

            # Build items list from candidate names and check group size before LLM processing
            items_for_this_group = []
            for name in candidate_names:
                normalized_name = name.lower()
                if normalized_name in names_lookup:
                    items_for_this_group.extend(names_lookup[normalized_name])
            if len(items_for_this_group) <= 1:
                logger.info(
                    f"Skipping group with candidate names {candidate_names} due to insufficient items (found {len(items_for_this_group)})."
                )
                continue

            # Only call the LLM if the group has enough items.
            representative_name = pick_representative_name_llm(candidate_names, organisation_type, web_search_results, client)
            group_uuid = str(uuid.uuid4())
            final_results[group_uuid] = {
                "name": representative_name,
                "items": items_for_this_group,
                "organisation_type": organisation_type
            }
    logger.info(f"Produced formatted groups with {len(final_results)} groups.")
    return final_results

def pick_representative_name_llm(candidate_names, organisation_type, web_search_results, client):
    system_message = "You are an AI assistant that chooses the best representative name for a group " \
        "of organisation names, given their organisation type and any web search results. " \
        "Output must be a JSON object with a single key 'name' containing the chosen representative name in lowercase."
    web_results_str = ""
    for name in candidate_names:
        results = web_search_results.get(name, [])
        if results:
            result_strs = []
            for result in results:
                url = result.get('url', '')
                title = result.get('title', '')
                description = result.get('description', '')
                result_strs.append(f"Title: {title}\nURL: {url}\nDescription: {description}")
            combined = '\n\n'.join(result_strs)
            web_results_str += f"Search results for '{name}':\n{combined}\n\n"
        else:
            web_results_str += f"Search results for '{name}':\nNo results currently available.\n\n"

    prompt = f"""
Given these organisation names:
{', '.join(candidate_names)}

All are determined to be a/an {organisation_type}.
Based on the web results below, pick the single best 'representative name' in lowercase.
Output the result as JSON in the format: {{"name": "chosen name"}}.
Web search results:
{web_results_str}
"""
    chat_history = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
    ]
    completion = client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=chat_history,
        response_format=RepresentativeNameResponse,
    )
    best_name = completion.choices[0].message.parsed.name
    if not best_name:
        best_name = candidate_names[0].lower()
    return best_name

# --- End of stages/stage9.py ---


# --- Start of stages/utils.py ---

import os
import sys
import time
import random
import logging
from pathlib import Path
import yaml
from pydantic import BaseModel

class GroupResponse(BaseModel):
    selected_names: list[str]

# Edit for using Openai GPT-4o
from openai import OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def get_client():
    return client
# def get_client():
#     return None

logger = logging.getLogger(__name__)

# Determine the project root based on the location of this file
# utils.py is at: OrgSync/src/local_llm/llama_v3/stages/utils.py
PROJECT_ROOT = Path(__file__).resolve().parents[4]  # Go up 4 levels
CONFIG_PATH = PROJECT_ROOT / 'cfg' / 'config.yaml'

# Load the configuration
with open(CONFIG_PATH, 'r') as f:
    config_data = yaml.safe_load(f)

MODELS_DIR = Path(config_data['models_dir']).resolve()
DEFAULT_CKPT_DIR = config_data['default_ckpt_dir']
TOKENIZER_PATH = MODELS_DIR / config_data['tokenizer_subpath']

# Add the 'models' directory to sys.path
sys.path.append(str(MODELS_DIR.parent))

# Set environment variables required by torch.distributed
os.environ['RANK'] = '0'
os.environ['WORLD_SIZE'] = '1'
os.environ['MASTER_ADDR'] = 'localhost'
os.environ['MASTER_PORT'] = '12355'  # You can choose any free port

def configure_environment():
    sys.path.append(str(MODELS_DIR.parent))
    os.environ['RANK'] = '0'
    os.environ['WORLD_SIZE'] = '1'
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

# Try importing search modules and model
try:
    from googlesearch import search as google_search
except ImportError:
    google_search = None
    logger.warning("Google search module not available.")

try:
    from models.llama3.reference_impl.generation import Llama
    from models.llama3.api.datatypes import (
        UserMessage,
        SystemMessage,
        CompletionMessage,
        StopReason
    )
except ModuleNotFoundError as e:
    logger.critical(f"Error importing Llama model modules: {e}")
    sys.exit(1)

try:
    from duckduckgo_search import DDGS
except ImportError:
    DDGS = None
    logger.warning("DuckDuckGo search module not available.")

logger.info("Initializing the Llama generator.")
generator = Llama.build(
    ckpt_dir=str(DEFAULT_CKPT_DIR),
    tokenizer_path=str(TOKENIZER_PATH),
    max_seq_len=8192,
    max_batch_size=4,
    model_parallel_size=None,
)

def get_generator():
    return generator

def perform_web_search(names, num_results=3, max_retries=5, search_method='duckduckgo', api_key=None):
    if search_method == 'duckduckgo' and DDGS is None:
        logger.error("DuckDuckGo search module not available.")
        sys.exit(1)

    web_search_results = {}
    search_count = 0  # Counter to track the number of searches

    for name in names:
        search_count += 1  # Increment the search count
        retries = 0
        success = False

        while retries < max_retries and not success:
            try:
                query = f'"{name}"'
                search_results = []
                ddgs = DDGS()
                results = ddgs.text(query, region='wt-wt', safesearch='Moderate', max_results=num_results)
                
                if results:
                    for res in results:
                        search_results.append({
                            'url': res.get('href', ''),
                            'title': res.get('title', ''),
                            'description': res.get('body', '')
                        })
                
                success = True

            except Exception as e:
                retries += 1
                logger.error(f"Error during web search for '{name}': {e}. Retrying ({retries}/{max_retries})...")
                time.sleep(64 * retries)
                
        if not success:
            logger.error(f"Failed to retrieve search results for '{name}' after {retries} retries.")
            web_search_results[name] = []
        else:
            web_search_results[name] = search_results

        # Pause for one minute if the search count is a multiple of 20
        if (search_count +1) % 20 == 0 and search_method=='duckduckgo':
            logger.info(f"Pausing for 1 minute after {search_count} searches...")
            time.sleep(60)

    return web_search_results

# --- End of stages/utils.py ---

